---
title: "fastml: Simplifying Machine Learning Workflows in R"
author: "Selcuk Korkmaz and Dincer Goksuluk"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: cerulean
    toc: true
    toc_float:
      collapsed: true

vignette: >
  %\VignetteIndexEntry{fastml: Simplifying Machine Learning Workflows in R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## 1. Introduction
fastml is an R package designed to simplify and streamline the process of training machine learning models. It leverages the tidymodels ecosystem to provide a user-friendly interface for model training, evaluation, and prediction, allowing both beginners and experienced practitioners to build robust models with minimal code.

This vignette will guide you through the usage of the fastml package, starting from simple examples and progressively moving to more complex scenarios, including custom tuning of hyperparameters.

Comprehensive Dependency Packages List
1. Core Dependencies
These packages are fundamental to the operation of the train_models function and the overall FastML package.

magrittr

Purpose: Provides the pipe operator (%>%) for chaining commands.
Functions Used: %>%
dplyr

Purpose: Data manipulation and transformation.
Functions Used: filter, mutate, select, bind_rows, pull, bind_cols
rlang

Purpose: Tools for programming with R, especially for handling non-standard evaluation.
Functions Used: sym, syms
dials

Purpose: Handles tuning parameters and their ranges.
Functions Used: range_set, value_set, grid_regular, grid_latin_hypercube
parsnip

Purpose: Defines and manages model specifications.
Functions Used: fit, extract_parameter_set_dials
workflows

Purpose: Manages modeling workflows, combining recipes and models.
Functions Used: workflow, add_model, add_recipe
tune

Purpose: Handles hyperparameter tuning.
Functions Used: tune_grid, control_grid, select_best, finalize_workflow, finalize_model
yardstick

Purpose: Provides tools for measuring model performance.
Functions Used: metric_set, accuracy, kap, roc_auc, sens, spec, precision, f_meas, rmse, rsq, mae
rsample

Purpose: Facilitates resampling methods for model evaluation.
Functions Used: vfold_cv, bootstraps, validation_split
recipes

Purpose: Manages data preprocessing steps.
Functions Used: recipe, step_dummy, step_center, step_scale
tibble

Purpose: Provides modern data frames.
Functions Used: is_tibble
ggplot2

Purpose: Data visualization.
Functions Used: ggplot, aes, geom_bar, facet_wrap, theme_bw, theme, element_text, labs
reshape2

Purpose: Data reshaping (melting and casting).
Functions Used: melt, dcast
stats

Purpose: Base R functions for statistical calculations.
Functions Used: predict
2. Algorithm-Specific Dependencies
These packages are required to train specific machine learning algorithms supported by FastML.

ranger

Purpose: Implements random forest models.
Algorithms Supported: random_forest
xgboost

Purpose: Provides gradient boosting models.
Algorithms Supported: xgboost
C50

Purpose: Implements the C5.0 decision tree algorithm.
Algorithms Supported: c5.0
e1071

Purpose: Supports Support Vector Machines (SVM) and other algorithms.
Algorithms Supported: svm_linear, svm_radial
gbm

Purpose: Implements generalized boosted regression models.
Algorithms Supported: gbm
nnet

Purpose: Provides functions for neural networks.
Algorithms Supported: neural_network
kernlab

Purpose: Offers kernel-based machine learning methods.
Algorithms Supported: Alternative for SVM (svm_radial)
glmnet

Purpose: Implements elastic net regularization.
Algorithms Supported: elastic_net
rpart

Purpose: Implements recursive partitioning for decision trees.
Algorithms Supported: decision_tree
MASS

Purpose: Provides functions for linear and quadratic discriminant analysis.
Algorithms Supported: lda, qda
pls

Purpose: Implements Partial Least Squares regression.
Algorithms Supported: pls
lightgbm

Purpose: Gradient boosting framework by Microsoft.
Algorithms Supported: lightgbm
keras

Purpose: Interface to Keras for deep learning models.
Algorithms Supported: deep_learning
baguette

Purpose: Provides bagging methods for tree-based models.
Algorithms Supported: bagging
kknn

Purpose: Implements k-Nearest Neighbors.
Algorithms Supported: kknn
klaR

Purpose: Provides functions for classification and clustering.
Algorithms Supported: klaR
discrim

Purpose: Implements discriminant analysis.
Algorithms Supported: discrim
bonsai

Purpose: Implements pruning methods for tree-based models.
Algorithms Supported: bonsai

## 2. Installation
Before using fastml, you need to install it and its dependencies. Since fastml relies on the tidymodels suite, make sure it's installed as well.

```{r eval=FALSE}
# Install tidymodels if not already installed
install.packages("tidymodels")
```

```{r eval=FALSE}
# Install from CRAN 
install.packages("fastml")  

# If installing from GitHub
install.packages("devtools")
devtools::install_github("selcukorkmaz/fastml")  
```


Load the necessary libraries:

```{r}
library(fastml)
library(tidymodels)
library(bonsai)
library(klaR)
library(discrim)
library(kknn)
library(rpart)
library(baguette)
library(mixOmics)
library(plsmod)

# reticulate::use_python("C:/Users/Sel√ßuk/AppData/Local/Programs/Python/Python312/", required = TRUE)
# tensorflow::install_tensorflow()
```

## 3. Getting Started with fastml
### Simple Classification Example
We'll begin by demonstrating how to use fastml for a simple classification task using the famous iris dataset.

```{r}
# Load the iris dataset
data(iris)
iris <- iris[iris$Species != "setosa", ]  # Binary classification
iris$Species <- factor(iris$Species)
```


```{r}
# View the first few rows
head(iris)
```

```{r}
# View classes of iris
table(iris$Species)
```

#### Training a Model with Default Settings

With fastml, you can train a model with minimal code:

```{r}
model <- fastml(
  data = iris,
  label = "Species"
)
```
By default, fastml will:

* Detect the task type (classification or regression) based on the target variable.

* Use default algorithms (16 classification algorithms) suitable for the task.

* Perform necessary preprocessing steps.

* Viewing the Model Summary

* You can get a summary of the trained models and their performance:

```{r}
# View the summary
summary(model)
```

This will display:

* The best model selected based on performance.

* Performance metrics for all trained models.

* Hyperparameters of the best model.

* A comparison plot of model performances.

#### Making Predictions

To make predictions on new data:

```{r}
# Predict on new data (using a subset of the iris dataset as an example)
new_data <- iris[1:5, -5]  # Exclude the target variable
predictions <- predict(model, newdata = new_data)
predictions
```

### Simple Regression Example
Now, let's see how fastml handles regression tasks using the mtcars dataset.

```{r}
# Load the mtcars dataset
data(mtcars)

# View the first few rows
head(mtcars)
```


Training a Regression Model

```{r}
# Train a regression model
model_reg <- fastml(
  data = mtcars,
  label = "mpg"
)
```

```{r}
# View the summary
summary(model_reg)
```



Making Predictions

```{r}
# Predict on new data
new_data_reg <- mtcars[1:5, -1]  # Exclude the target variable
predictions_reg <- predict(model_reg, newdata = new_data_reg)
predictions_reg
```

### Customizing fastml

####Specifying Algorithms

You can specify which algorithms to use by providing a vector of algorithm names:

```{r}
# Train models with specific algorithms
model_custom_algos <- fastml(
  data = iris,
  label = "Species",
  algorithms = c("random_forest", "xgboost", "svm_linear")
)
```

```{r}
# View the summary
summary(model_custom_algos)
```

<!-- Using a Custom Recipe -->
<!-- If you want to include specific preprocessing steps, you can create a custom recipe: -->

<!-- r -->
<!-- Kodu kopyala -->
<!-- # Load required packages -->
<!-- library(recipes) -->

<!-- # Create a custom recipe -->
<!-- custom_recipe <- recipe(Species ~ ., data = iris) %>% -->
<!--   step_normalize(all_numeric_predictors()) %>% -->
<!--   step_pca(all_numeric_predictors(), num_comp = 2) -->

<!-- # Train the model with the custom recipe -->
<!-- model_custom_recipe <- fastml( -->
<!--   data = iris, -->
<!--   label = "Species", -->
<!--   recipe = custom_recipe -->
<!-- ) -->

<!-- # View the summary -->
<!-- summary(model_custom_recipe) -->

#### Setting Custom Resampling Methods
You can specify different resampling methods for cross-validation:

```{r}
# Train models with repeated cross-validation
model_repeated_cv <- fastml(
  data = iris,
  label = "Species",
  resampling_method = "repeatedcv",
  folds = 5,
  repeats = 3
)
```
```{r}
# View the summary
summary(model_repeated_cv)
```



### Advanced Usage

#### Custom Hyperparameter Tuning

fastml allows you to define custom hyperparameter tuning grids for each algorithm.

##### Defining Custom Tuning Parameters

```{r}
# Define custom tuning parameters for random forest
tune_params_rf <- list(
  random_forest = list(
    mtry = c(1, 4),
    trees = c(50, 200),
    min_n = c(2, 10)
  )
)
```

```{r}
# Train the model with custom tuning parameters
model_custom_tune <- fastml(
  data = iris,
  label = "Species",
  algorithms = "random_forest",
  tune_params = tune_params_rf,
  use_default_tuning = TRUE  # Use default tuning grids for other algorithms if needed
)
```


```{r}
# View the summary
summary(model_custom_tune)
```


